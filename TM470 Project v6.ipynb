{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe037b9",
   "metadata": {},
   "source": [
    "# TM470 Project - Automating the Identification of UK Coarse Fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c501176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as et # https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "from tensorflow.python.client import device_lib #for detection of devices\n",
    "from tensorflow.keras import Sequential, optimizers, metrics, layers\n",
    "# for model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b4078",
   "metadata": {},
   "source": [
    "//3 Is TF using GPU acceleration from inside python shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is TF using GPU?\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "# Number of GPU's available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Details of CPU and GPU from the device library (device_lib)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List AFFiNe dataset from Kaggle datasets\n",
    "!kaggle datasets list -s jorritvenema/AFFiNe\n",
    "# List files in the AFFiNe dataset through Kaggle api\n",
    "!kaggle datasets files jorritvenema/AFFiNe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip dataset folder (only run once)\n",
    "#!kaggle datasets download jorritvenema/AFFiNe --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eada694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70948435",
   "metadata": {},
   "source": [
    "# Where to find the test data\n",
    "    \"base_dir = '/datasets/dogs-vs-cats/'\\n\",\n",
    "    \"train_dir = os.path.join(base_dir, 'train')\\n\",\n",
    "    \"validation_dir = os.path.join(base_dir, 'validation')\\n\",\n",
    "    \"test_dir = os.path.join(base_dir, 'test')\\n\",\n",
    "    \"\\n\",\n",
    "    \"ROWS = 256\\n\",\n",
    "    \"COLS = 256\\n\",\n",
    "    \"CHANNELS = 3\\n\",\n",
    "    \"INPUT_SHAPE = (ROWS, COLS, CHANNELS)\\n\",\n",
    "    \"\\n\",\n",
    "    \"BATCH_SIZE = 64\"\n",
    "    #----------------------------------------------\n",
    "    #my code\n",
    "    # Dataset address is C:\\Users\\Rob\\Dataset\n",
    "    # datasetPath = (r'C:\\Users\\Rob\\Dataset')\n",
    "    # base_dir = pathlib.Path(datasetPath).with_suffix('')\n",
    "    # train_dir = os.path.join(base_dir, 'train')\n",
    "    # validation_dir = os.path.join(base_dir, 'validation')\n",
    "    # test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset address is C:\\Users\\Rob\\Dataset\n",
    "#datasetPath = (r'C:\\Users\\Rob\\Dataset')\n",
    "\n",
    "# Assigning dataset path to pathlib\n",
    "base_dir = pathlib.Path(r'C:\\Users\\Rob\\Dataset') #.with_suffix('')?\n",
    "print(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images in dataset\n",
    "image_count = len(list(base_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size\n",
    "batch_size=16\n",
    "img_height=180\n",
    "img_width=180\n",
    "image_size=(img_height,img_width,3)\n",
    "num_classes = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "  base_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height,img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation dataset\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "  base_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=124,\n",
    "  image_size=(img_height,img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c254c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test dataset\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "  base_dir,\n",
    "  #validation_split=0.2,\n",
    "  #subset=\"validation\",\n",
    "  seed=125,\n",
    "  image_size=(img_height,img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e300cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the class names\n",
    "class_names = train_dataset.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02dfd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some images from the training dataset\n",
    "plt.figure(figsize=(9, 9))\n",
    "for images, labels in train_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b5b09",
   "metadata": {},
   "source": [
    "# Configure the dataset for performance, caching and prefetching\n",
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa754ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset for performance\n",
    "#AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "#train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08f505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bd60b02",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243163a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.tensorflow.org/tutorials/load_data/images\n",
    "# Create model\n",
    "#\n",
    "#\n",
    "#model = tf.keras.Sequential([\n",
    "#  tf.keras.layers.Rescaling(1./255),\n",
    "#  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#  tf.keras.layers.MaxPooling2D(),\n",
    "#  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#  tf.keras.layers.MaxPooling2D(),\n",
    "#  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#  tf.keras.layers.MaxPooling2D(),\n",
    "#  tf.keras.layers.Flatten(),\n",
    "#  tf.keras.layers.Dense(128, activation='softmax'),\n",
    "#  tf.keras.layers.Dense(num_classes)\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af67a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "#model.compile(\n",
    "#  optimizer='adam',\n",
    "#  #optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537b83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2967bc5b",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist=model.fit(\n",
    "#    train_dataset, \n",
    "#    validation_data=val_dataset, \n",
    "#    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5df6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss and accuracy as well as validation loss and accuracy over the number of epochs\n",
    "#hist_dict = hist.history\n",
    "\n",
    "# obtain the accuracy and loss of the training set and verification set in the returned\n",
    "#train_acc = hist.history['accuracy']\n",
    "#val_acc = hist.history['val_accuracy']\n",
    "#train_loss = hist.history['loss']\n",
    "#val_loss = hist.history['val_loss']\n",
    "\n",
    "#epochs = range(1, len(train_acc)+1)\n",
    "#plt.plot(epochs, train_acc, 'bo', label = 'Training acc')\n",
    "#plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "#plt.title('Training and validation accuracy')\n",
    "#plt.legend() # show legend \n",
    "#plt.xlabel('Epochs')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.show()\n",
    "#plt.figure()\n",
    "\n",
    "#plt.plot(epochs, train_loss, 'bo', label = 'Training loss')\n",
    "#plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "#plt.title('Training and validation loss')\n",
    "#plt.legend()\n",
    "#plt.xlabel('Epochs')\n",
    "#plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aed06a",
   "metadata": {},
   "source": [
    "# My TM358 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the normalisation layer\n",
    "norm_layer = layers.Normalization(input_shape=(image_size))\n",
    "norm_layer.adapt(train_dataset.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an augmented subset\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "layers.RandomRotation(0.2),\n",
    "#layers.RandomZoom(height_factor=0.1),\n",
    "layers.RandomFlip(mode='horizontal')\n",
    "])\n",
    "\n",
    "aug_train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "num_parallel_calls=tf.data.AUTOTUNE)\n",
    "aug_train_dataset = aug_train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        #norm_layer, # to normalise data - making training freeze\n",
    "        Conv2D(filters=16, kernel_size=(3,3), padding='same',\n",
    "        input_shape=image_size, activation='relu'),\n",
    "        Conv2D(filters=16, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        Conv2D(filters=16, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=16, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), padding='valid', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',#(learning_rate=0.005),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model using the build_model function\n",
    "model=build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496529dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "#with tf.device(\"/device:GPU:0\"):\n",
    "hist=model.fit(\n",
    "aug_train_dataset, \n",
    "validation_data=val_dataset, \n",
    "verbose=1,\n",
    "epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss and accuracy as well as validation loss and accuracy over the number of epochs\n",
    "hist_dict = hist.history\n",
    "\n",
    "# obtain the accuracy and loss of the training set and verification set in the returned\n",
    "train_acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "train_loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(train_acc)+1)\n",
    "plt.plot(epochs, train_acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend() # show legend \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd08c2d",
   "metadata": {},
   "source": [
    "## From TM358 EMA Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5598c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions=model.predict(test_dataset)\n",
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebe233",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels=np.array(list(test_dataset.unbatch().map(lambda x,y: y).as_numpy_iterator()))\n",
    "actual_labels=np.argmax(actual_labels, axis=0)\n",
    "actual_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b07c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_imgs, sample_labels = test_dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3360374",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions = model(sample_imgs)\n",
    "# View the true and predicted labels of sample images\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(15):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(sample_imgs[i].astype(\"uint8\"))\n",
    "    #plt.imshow(sample_imgs[i])\n",
    "    p_class = np.argmax(sample_predictions[i])\n",
    "    a_class = np.argmax(sample_labels[i])\n",
    "    plt.title(f\"P: {class_names[p_class]}\\n(A: {class_names[a_class]})\",\n",
    "    color=(\"green\" if p_class == a_class else \"red\"))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbc366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
