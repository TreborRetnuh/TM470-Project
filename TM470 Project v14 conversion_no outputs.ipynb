{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe037b9",
   "metadata": {},
   "source": [
    "# TM470 Project - Automating the Identification of UK Coarse Fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c501176",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit #scikit-learn.org\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as et # https://docs.python.org/3/library/xml.etree.elementtr\n",
    "from tensorflow.python.client import device_lib #for detection of devices\n",
    "import glob as glob # Searches for certain files\n",
    "# for model\n",
    "import keras\n",
    "from tensorflow.keras import Sequential, optimizers, metrics, layers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Rescaling\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641c79e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b4078",
   "metadata": {},
   "source": [
    "### 3 Is TF using GPU acceleration from inside python shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551ad11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Is TF using GPU?\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "# Number of GPU's available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Details of CPU and GPU from the device library (device_lib)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fadafc",
   "metadata": {},
   "source": [
    "### AFFiNe dataset from Kaggle (list, download and unzip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14aef8f",
   "metadata": {},
   "source": [
    "### Classes taken out of original dataset\n",
    "Aspius aspius Asp, Carassius gibelio (Carp Prussian), Lepomis gibbosus Pumpkinseed, Neogobius fluviatilis Goby (monkey), Neogobius kessleri Goby (bighead), Neogobius melanostomus (Goby Round), Rhodeus amarus Bitterling (European), Vimba vimba Vimba, Leuciscus leuciscus Dace, Gasterosteus aculeatus Stickleback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db062022",
   "metadata": {},
   "source": [
    "### Assigning filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b8a01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# AFFiNe dataset from Kaggle placed in Jupyter folder\n",
    "# https://www.kaggle.com/datasets/jorritvenema/affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dc496",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasetPath = 'UK AFFiNe Split/Main'\n",
    "testDatasetPath = 'UK AFFiNe Split/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da455ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasetPath, testDatasetPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62034b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assigning dataset path to pathlib\n",
    "dat_dir = pathlib.Path(datasetPath).with_suffix('')\n",
    "print(dat_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dat_dir = pathlib.Path(testDatasetPath).with_suffix('')\n",
    "print(test_dat_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9ca39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of images in Main dataset\n",
    "image_count = len(list(dat_dir.glob('*/*.jpg'))) # is this how datasetPath should be?\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f97f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images Test in dataset\n",
    "image_count = len(list(test_dat_dir.glob('*/*.jpg'))) # is this how datasetPath should be?\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9e8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b18c160",
   "metadata": {},
   "source": [
    "# Get class names and bound box information from XML files using the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb9099",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reading the information in the XML files and extracting names/bounding box info\n",
    "path = (dat_dir)\n",
    "filelist = []\n",
    "list1 = list()\n",
    "list2 = list()\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if not file.endswith('.xml'):\n",
    "            continue\n",
    "        filelist.append(os.path.join(root, file))\n",
    "for file in filelist:\n",
    "    root = et.parse(file).getroot() # get the root of the xml\n",
    "# Get class names\n",
    "    for className in root.findall('.//object'):\n",
    "        class_name = className.find('name').text\n",
    "        data = np.array([class_name])\n",
    "    list1.append(data)\n",
    "# Get bounding box information\n",
    "    for bndBox in root.findall('.//object'):\n",
    "        bounding_box = bndBox.find('bndbox').text\n",
    "        xmin = int(bndBox.find('./bndbox/xmin').text)\n",
    "        ymin = int(bndBox.find('./bndbox/ymin').text)\n",
    "        xmax = int(bndBox.find('./bndbox/xmax').text)\n",
    "        ymax = int(bndBox.find('./bndbox/ymax').text)\n",
    "        data2 = np.array([xmin,ymin,xmax,ymax])\n",
    "    list2.append(data2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f7f9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f211bd",
   "metadata": {},
   "source": [
    "## Create dataframe (using relative paths, class names and bound box details from XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3eac5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#list(base_dir.glob('*/*.jpg'))\n",
    "filepaths = list(dat_dir.glob(r'**/*.jpg'))\n",
    "classnames = list1#list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "boundboxes = list2\n",
    "\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)#str\n",
    "classnames = pd.Series(classnames, name='Class Name')\n",
    "boundboxes = pd.Series(boundboxes, name='Boundbox')\n",
    "\n",
    "dataframe1 = pd.concat([filepaths , classnames, boundboxes] , axis=1)\n",
    "dataframe1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f407aca",
   "metadata": {},
   "source": [
    "### Class counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b10f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# species counts for each class with UK AFFiNe\n",
    "\n",
    "dataframe1['Class Name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228df27",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Images count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583ec5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of images in dataset and dataframe1\n",
    "image_count = len(list(dat_dir.glob('*/*.jpg')))\n",
    "image_count_df = len(dataframe1)\n",
    "print(image_count)\n",
    "print(image_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40170510",
   "metadata": {},
   "source": [
    "## Creating the datasets (looking at stratified shuffle split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c7f19",
   "metadata": {},
   "source": [
    "### (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = list(dat_dir.glob(r'**/*.jpg'))\n",
    "#y = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], X))\n",
    "\n",
    "#sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, train_size=0.2, random_state=0)\n",
    "#sss.get_n_splits(X, y)\n",
    "\n",
    "#print(sss)\n",
    "\n",
    "#for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "#    print(f\"Fold {i}:\")\n",
    "#    print(f\"  Train: index={train_index}\")\n",
    "#    print(f\"  Test:  index={test_index}\")\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True)\n",
    "#X_train, X_val, y_train, y_val =train_test_split(X_train,y_train,test_size=0.25,random_state=0) # 0.25 x 0.8 0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b41e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f736f27",
   "metadata": {},
   "source": [
    "# Creating datasets using image dataset from directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0809ad62",
   "metadata": {},
   "source": [
    "### Assigning batch and image sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881c6d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Image size\n",
    "batch_size=32\n",
    "img_height=256\n",
    "img_width=256\n",
    "img_size=(img_height, img_width,3)\n",
    "num_classes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c9cfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the training dataset\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "  dat_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  shuffle=True,\n",
    "  image_size=(img_height, img_width),\n",
    "  #color_mode='rgb',\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6b2e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the validation dataset\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "  dat_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  shuffle=True,\n",
    "  image_size=(img_height, img_width),\n",
    "  #color_mode='rgb',\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5bdca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test dataset\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_dat_dir,\n",
    "  #validation_split=0.6,\n",
    "  #subset=\"validation\",\n",
    "  #seed='123',\n",
    "  shuffle = True,\n",
    "  image_size=(img_height, img_width),\n",
    "  #color_mode='rgb',\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed93fcd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assign the class names\n",
    "class_names = test_dataset.class_names#test_dataset\n",
    "#class_names=list1\n",
    "print(class_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46603a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Next two cells for testing\n",
    "sample_imgs, sample_labels = test_dataset.as_numpy_iterator().next()\n",
    "sample_imgs.shape, sample_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742d227",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Show sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410a8f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing using sample label - to try debug final evaluation\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(sample_imgs[i].astype(\"uint8\")) #images[i].numpy().astype(\"uint8\"))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.title(class_names[sample_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aed06a",
   "metadata": {},
   "source": [
    "## My model (based on TM358 EMA model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfdf19",
   "metadata": {},
   "source": [
    "### Normalisation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165686b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating the normalisation layer\n",
    "norm_layer = layers.Normalization(input_shape=(img_size))\n",
    "norm_layer.adapt(train_dataset.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a88897",
   "metadata": {},
   "source": [
    "### Augmenting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf23c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating an augmented subset\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "#layers.RandomRotation(0.25),#- worse accuracy (but what about overfitting?) causes freezing at 0.1+\n",
    "#layers.RandomZoom(height_factor=0.2), # testing cause of model fit freeze\n",
    "layers.RandomFlip(mode='horizontal'),\n",
    "layers.RandomFlip(mode='vertical'),# worse but not having it results in overfitting\n",
    "])\n",
    "\n",
    "aug_train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "num_parallel_calls=tf.data.AUTOTUNE)\n",
    "aug_train_dataset = aug_train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220dfa6",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = tf.keras.optimizers.Adam(learning_rate=0.0001)#learning_rate=0.0001,or 3e-4\n",
    "def build_model():\n",
    "      model = Sequential([\n",
    "          #norm_layer, \n",
    "          Conv2D(filters=64, kernel_size=(3,3), padding=\"same\",input_shape=(img_size), activation= \"relu\"), \n",
    "          Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "          MaxPooling2D(pool_size=(2,2)),\n",
    "          Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "          Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "          MaxPooling2D(pool_size=(2,2)),\n",
    "          Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "          Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "          MaxPooling2D(pool_size=(2,2)),\n",
    "          Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "          Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "          MaxPooling2D(pool_size=(2,2)),\n",
    "          Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "          Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "          MaxPooling2D(pool_size=(2,2)),\n",
    "          Dropout(0.5),\n",
    "          Flatten(),\n",
    "          Dense(512, activation='relu'),# num_classes*25 = 500\n",
    "          Dropout(0.5),\n",
    "          Dense(20, activation='softmax')#num_classes * 1.5 or 20 * 1. \n",
    "      ])\n",
    "      model.compile(\n",
    "          optimizer=ada,#'adam',#learning_rate=0.0001,or 3e-4\n",
    "          loss='sparse_categorical_crossentropy',#sparse_categorical_crossentropy\n",
    "          metrics=['accuracy']\n",
    "          )\n",
    "      return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model using the build_model function\n",
    "model=build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2aa0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf05a07",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "#with tf.device(\"/cpu:0\"):\n",
    "#with tf.device(\"/device:GPU:0\"):\n",
    "hist=model.fit(\n",
    "aug_train_dataset,# aug_train_dataset\n",
    "validation_data=val_dataset, \n",
    "verbose=1,\n",
    "#shuffle=True,\n",
    "epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea15c8",
   "metadata": {},
   "source": [
    "### Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4dece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss and accuracy as well as validation loss and accuracy over the number of epochs\n",
    "hist_dict = hist.history\n",
    "\n",
    "# obtain the accuracy and loss of the training set and verification set in the returned\n",
    "train_acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "train_loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(train_acc)+1)\n",
    "plt.plot(epochs, train_acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend() # show legend \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498b18b",
   "metadata": {},
   "source": [
    "### Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48511d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions = model(sample_imgs)\n",
    "# View the true and predicted labels of sample images\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(sample_imgs[i].astype(\"uint8\"))\n",
    "    #plt.imshow(sample_imgs[i])\n",
    "    p_class = np.argmax(sample_predictions[i])\n",
    "    a_class = sample_labels[i]# a_class = np.argmax(sample_labels[i]) ##np.argmax was the problem?!?\n",
    "    #plt.title(f\"P: {class_names[p_class]}\\n(A: {class_names[a_class]})\",\n",
    "    plt.title(f\"P: {class_names[p_class]}\\n(A: {class_names[a_class]})\",# class_names[a_class]\n",
    "    color=(\"green\" if p_class == a_class else \"red\"))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0293f8",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee994526",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model.h5')\n",
    "\n",
    "with open('saved_model_history.json', 'w') as f:\n",
    "    json.dump(hist.history, f)\n",
    "    \n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631ac60",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model.h5')\n",
    "\n",
    "with open('saved_model_history.json') as f:\n",
    "    example_history = json.load(f)\n",
    "\n",
    "print('model loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df941b9e",
   "metadata": {},
   "source": [
    "### Convert model to TF Lite and save as TF Lite model\n",
    "### (https://www.tensorflow.org/lite/models/convert/convert_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952498f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"model.tflite\", 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
